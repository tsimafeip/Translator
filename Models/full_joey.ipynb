{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_joey.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMnmQBBf423e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1add10-07d9-4fe5-9b67-d331127cff89"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(\"/content/Separate Corpuses/FullCorpus/\") or not os.path.isdir(\"/joey_experiments/\"):\n",
        "    !gsutil -m cp -r \"gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/\" .\n",
        "    !gsutil -m cp -r \"gs://mytranslator-298419-vcm/joey_experiments/\" ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Bel.txt...\n",
            "/ [0/8 files][    0.0 B/651.5 MiB]   0% Done                                    \rCopying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Rus.txt_tc.model...\n",
            "/ [0/8 files][    0.0 B/651.5 MiB]   0% Done                                    \rCopying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Bel.txt_tok.txt...\n",
            "/ [0/8 files][    0.0 B/651.5 MiB]   0% Done                                    \rCopying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Rus.txt_tok.txt...\n",
            "/ [0/8 files][    0.0 B/651.5 MiB]   0% Done                                    \rCopying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Bel.txt_tc.model...\n",
            "/ [0/8 files][    0.0 B/651.5 MiB]   0% Done                                    \rCopying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Bel.txt_tok.txt_true.txt...\n",
            "/ [0/8 files][    0.0 B/651.5 MiB]   0% Done                                    \rCopying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Rus.txt...\n",
            "Copying gs://mytranslator-298419-vcm/RuBeCorpus_BelapanNews/Separate Corpuses/FullCorpus/Corpus_Rus.txt_tok.txt_true.txt...\n",
            "- [8/8 files][651.5 MiB/651.5 MiB] 100% Done                                    \n",
            "Operation completed over 8 objects/651.5 MiB.                                    \n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/train.bpe.be...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/bpe.codes.5000...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/train.bpe.ru...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/dev.bpe.ru...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/dev.bpe.be...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/test.bpe.be...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/test.bpe.ru...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/vocab.be...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/vocab.ru...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/data/vocab.txt...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/models/best.ckpt...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/models/config.yaml...\n",
            "Copying gs://mytranslator-298419-vcm/joey_experiments/ru-be-new_full/models/config_non_cuda.yaml...\n",
            "| [13/13 files][412.3 MiB/412.3 MiB] 100% Done                                  \n",
            "Operation completed over 13 objects/412.3 MiB.                                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz1REQDJABAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05abbbe8-953f-4a3f-b615-8621bdb8c9e4"
      },
      "source": [
        "import os\n",
        "from os import path\n",
        "source_language = \"ru\"\n",
        "target_language = \"be\"\n",
        "lang_pair = source_language+target_language\n",
        "tag = \"new_full\"\n",
        "model_type = \"bpe\"\n",
        "\n",
        "use_cuda = False\n",
        "\n",
        "if model_type == \"bpe\":\n",
        "    # Learn BPEs on the training data.\n",
        "    number_of_splits = 5000\n",
        "    os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language)\n",
        "    bpe_codes_file = \"bpe.codes.\"+str(number_of_splits)\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"lang_pair\"] = lang_pair\n",
        "os.environ[\"tag\"] = tag\n",
        "os.environ[\"model_type\"] = model_type\n",
        "\n",
        "gdrive_path = \"/content/joey_experiments/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "os.environ[\"gdrive_path\"] = gdrive_path\n",
        "!echo $gdrive_path"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/joey_experiments/ru-be-new_full\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtyNflSkCl0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0158f9-b154-47d9-ca66-a8c2fc56c38d"
      },
      "source": [
        "raw_source_file = r\"/content/FullCorpus/Corpus_Rus.txt\"\n",
        "raw_target_file = r\"/content/FullCorpus/Corpus_Bel.txt\"\n",
        "\n",
        "# They should both have the same length.\n",
        "! wc -l \"$raw_source_file\"\n",
        "! wc -l \"$raw_target_file\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "429479 /content/FullCorpus/Corpus_Rus.txt\n",
            "429479 /content/FullCorpus/Corpus_Bel.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqdEumRjUuNG"
      },
      "source": [
        "%%capture\n",
        "! pip install sacremoses"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYcEmGKuTXf0"
      },
      "source": [
        "def preprocess(filepath, language, force_preprocess=False, pretrained_tc_model=None):\n",
        "  tok_file = filepath+\"_tok.txt\"\n",
        "  tc_model = pretrained_tc_model if pretrained_tc_model else filepath+\"_tc.model\"\n",
        "  truecased_file = tok_file+\"_true.txt\"\n",
        "\n",
        "  if force_preprocess:\n",
        "    ! sed -i 's/\"//g' \"$filepath\"\n",
        "    ! sacremoses tokenize -l language -e 'utf-8' < \"$filepath\" > \"$tok_file\"\n",
        "    if not pretrained_tc_model:\n",
        "      ! sacremoses train-truecase -m \"$tc_model\" -j 4 < \"$tok_file\"\n",
        "    ! sacremoses truecase -m \"$tc_model\" -j 4 < \"$tok_file\" > \"$truecased_file\"\n",
        "\n",
        "  return truecased_file, tc_model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_9CZwEccDeT"
      },
      "source": [
        "# Change the pointers to our files such that we continue to work with the tokenized and truecased data.\n",
        "source_file, source_tc_model = preprocess(raw_source_file, source_language)\n",
        "target_file, target_tc_model = preprocess(raw_target_file, target_language)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX6AsQDypMMP"
      },
      "source": [
        "%%capture\n",
        "# installing cuda\n",
        "if use_cuda:\n",
        "    !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "    !sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "    !sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\n",
        "    !sudo add-apt-repository \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/ /\"\n",
        "    !sudo apt-get update\n",
        "    !sudo apt-get -y install cuda"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eVT5XdS9D_v"
      },
      "source": [
        "# #Let's take a look what preprocessing did to the text.\n",
        "# ! head \"$raw_source_file\"*\n",
        "# ! head \"$raw_target_file\"*"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExgM0rDoRoXZ"
      },
      "source": [
        "# # test data preparation\n",
        "# raw_source_test_file = r\"/content/drive/My Drive/joey_experiments/data/test.ru\"\n",
        "# raw_target_test_file = r\"/content/drive/My Drive/joey_experiments/data/test.be\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBVaYRvUXhaa"
      },
      "source": [
        "# source_test_file, _model = preprocess(raw_source_test_file, source_language)\n",
        "# target_test_file, _model = preprocess(raw_target_test_file, target_language)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Z05xhfdGqe"
      },
      "source": [
        "# ! head \"$raw_source_test_file\"*\n",
        "# ! head \"$raw_target_test_file\"*"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpJ-z3HY5PdD"
      },
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# def create_df(source_f, target_f, lowercase_first_letter=True):\n",
        "#    source = []\n",
        "#    target = []\n",
        "#    with open(source_f) as f:\n",
        "#        for i, line in enumerate(f):\n",
        "#          if lowercase_first_letter:\n",
        "#            line = line[0].lower() + line[1:]\n",
        "#          source.append(line.strip())    \n",
        "   \n",
        "#    with open(target_f) as f:\n",
        "#        for j, line in enumerate(f):\n",
        "#          if lowercase_first_letter:\n",
        "#            line = line[0].lower() + line[1:]\n",
        "#          target.append(line.strip())\n",
        "       \n",
        "#    df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "\n",
        "#    df_pp = df.drop_duplicates()\n",
        "#    df_pp = df_pp.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "#    #print(df.head(3))\n",
        "#    return df_pp"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYzj5rRAZHEM"
      },
      "source": [
        "# train_df = create_df(source_file, target_file)\n",
        "# test_df = create_df(source_test_file, target_test_file)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iSIC0WLHnhi"
      },
      "source": [
        "# # We use 60k for dev, rest (aprx. 6k) for test\n",
        "# import csv\n",
        "\n",
        "# num_train_patterns = int(len(train_df)*0.9)\n",
        "\n",
        "# train = train_df.head(num_train_patterns)\n",
        "# dev = train_df.drop(train.index)\n",
        "\n",
        "# def write_to_files(prefix, source):\n",
        "#   with open(f\"{prefix}.\"+source_language, \"w\") as src_file, open(f\"{prefix}.\"+target_language, \"w\") as trg_file:\n",
        "#     for index, row in source.iterrows():\n",
        "#       src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "#       trg_file.write(row[\"target_sentence\"]+\"\\n\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBpN9MdqaoA_"
      },
      "source": [
        "#write_to_files(\"train\", train)\n",
        "# write_to_files(\"dev\", dev)\n",
        "# write_to_files(\"test\", test_df)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj4W1EHiJFL5"
      },
      "source": [
        "# ! head train.*\n",
        "# ! head test.*"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku1Ws7IE6pdL"
      },
      "source": [
        "# Install JoeyNMT\n",
        "%%capture\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1GuIR88g3p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38f397f-44af-4bf6-ba06-2a8175780ce8"
      },
      "source": [
        "! cd joeynmt; python3 -m unittest"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "................................s.......................\n",
            "----------------------------------------------------------------------\n",
            "Ran 56 tests in 1.177s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrplk2FeXFsF"
      },
      "source": [
        "%%capture\n",
        "# ! pip install sentencepiece"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrCfWbZFJrEw"
      },
      "source": [
        "# import sentencepiece as spm\n",
        "\n",
        "# spm.SentencePieceTrainer.Train(f\"--input=train.{source_language} --model_prefix={source_language}_sentp_{model_type} --vocab_size=16000 --character_coverage=1.0 --model_type={model_type}\")\n",
        "# spm.SentencePieceTrainer.Train(f\"--input=train.{target_language} --model_prefix={target_language}_sentp_{model_type} --vocab_size=16000 --character_coverage=1.0 --model_type={model_type}\")\n",
        "\n",
        "# sp_source = spm.SentencePieceProcessor()\n",
        "# sp_source.Load(f\"{source_language}_sentp.model\")\n",
        "\n",
        "# sp_target = spm.SentencePieceProcessor()\n",
        "# sp_target.Load(f\"{target_language}_sentp.model\")\n",
        "\n",
        "# def encode(filename, source):\n",
        "#   with open(f\"{filename}.{model_type}.\"+source_language, \"w\") as src_file, open(f\"{filename}.{model_type}.\"+target_language, \"w\") as trg_file:\n",
        "#     for index, row in source.iterrows():\n",
        "#       src_file.write(\"\".join(sp_source.EncodeAsPieces(row[\"source_sentence\"]))+ \"\\n\")\n",
        "#       trg_file.write(\"\".join(sp_target.EncodeAsPieces(row[\"target_sentence\"]))+ \"\\n\")\n",
        "\n",
        "# encode(\"train\", train)\n",
        "# encode(\"dev\", dev)\n",
        "# encode(\"test\", test_df)\n",
        "\n",
        "# from os import path\n",
        "\n",
        "# os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", lang_pair)\n",
        "\n",
        "# # Create directory, move everyone we care about to the JOEY correct location\n",
        "# ! mkdir -p $data_path\n",
        "# ! cp train.* $data_path\n",
        "# ! cp test.* $data_path\n",
        "# ! cp dev.* $data_path\n",
        "# ! ls $data_path\n",
        "\n",
        "# # Create that vocab using build_vocab\n",
        "# ! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "# ! joeynmt/scripts/build_vocab.py $data_path/train.$model_type.$src $data_path/train.$model_type.$tgt --output_path $data_path/vocab.txt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmpzYUUSXWlY"
      },
      "source": [
        "# # Do subword NMT\n",
        "# ! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 5000 -o $bpe_codes_file --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# # Apply BPE splits to the development and test data.\n",
        "# ! subword-nmt apply-bpe -c $bpe_codes_file --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "# ! subword-nmt apply-bpe -c $bpe_codes_file --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "# ! subword-nmt apply-bpe -c $bpe_codes_file --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "# ! subword-nmt apply-bpe -c $bpe_codes_file --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "# ! subword-nmt apply-bpe -c $bpe_codes_file --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "# ! subword-nmt apply-bpe -c $bpe_codes_file --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# # Create directory, move everyone we care about to the correct location\n",
        "# ! mkdir -p $data_path\n",
        "# ! cp train.* $data_path\n",
        "# ! cp test.* $data_path\n",
        "# ! cp dev.* $data_path\n",
        "# ! cp $bpe_codes_file $data_path\n",
        "# ! ls $data_path\n",
        "\n",
        "# # Create that vocab using build_vocab\n",
        "# ! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "# ! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# # Some output\n",
        "# ! echo \"BPE Xhosa Sentences\"\n",
        "# ! tail -n 5 test.bpe.$tgt\n",
        "# ! echo \"Combined BPE Vocab\"\n",
        "# ! tail -n 10 joeynmt/data/$src$tgt/vocab.txt"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R9sezk2vBJN"
      },
      "source": [
        "# # Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "# ! cp train.* \"$gdrive_path\"\n",
        "# ! cp test.* \"$gdrive_path\"\n",
        "# ! cp dev.* \"$gdrive_path\"\n",
        "# ! cp $bpe_codes_file \"$gdrive_path\"\n",
        "# ! ls \"$gdrive_path\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLLzXxIaY_K8"
      },
      "source": [
        "# # This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# # (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "# # Create the config\n",
        "# config = \"\"\"\n",
        "# name: \"{name}_transformer\"\n",
        "\n",
        "# data:\n",
        "#     src: \"{source_language}\"\n",
        "#     trg: \"{target_language}\"\n",
        "#     train: \"data/{name}/train.{model_type}\"\n",
        "#     dev:   \"data/{name}/dev.{model_type}\"\n",
        "#     test:   \"data/{name}/test.{model_type}\"\n",
        "#     level: \"{model_type}\"\n",
        "#     lowercase: False\n",
        "#     max_sent_length: 130\n",
        "#     src_vocab: \"data/{name}/vocab.txt\"\n",
        "#     trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "# testing:\n",
        "#     beam_size: 5\n",
        "#     alpha: 1.0\n",
        "\n",
        "# training:\n",
        "#     #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "#     random_seed: 42\n",
        "#     optimizer: \"adam\"\n",
        "#     normalization: \"tokens\"\n",
        "#     adam_betas: [0.9, 0.999] \n",
        "#     scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "#     patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "#     learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "#     learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "#     decrease_factor: 0.7\n",
        "#     loss: \"crossentropy\"\n",
        "#     learning_rate: 0.0003\n",
        "#     learning_rate_min: 0.00000001\n",
        "#     weight_decay: 0.0\n",
        "#     label_smoothing: 0.1\n",
        "#     batch_size: 4096\n",
        "#     batch_type: \"token\"\n",
        "#     eval_batch_size: 3600\n",
        "#     eval_batch_type: \"token\"\n",
        "#     batch_multiplier: 1\n",
        "#     early_stopping_metric: \"ppl\"\n",
        "#     epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "#     validation_freq: 4000          # TODO: Set to at least once per epoch.\n",
        "#     logging_freq: 100\n",
        "#     eval_metric: \"bleu\"\n",
        "#     model_dir: \"models/{name}_transformer_new\"\n",
        "#     overwrite: True               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "#     shuffle: True\n",
        "#     use_cuda: {use_cuda}\n",
        "#     max_output_length: 130\n",
        "#     print_valid_sents: [0, 1, 2, 3]\n",
        "#     keep_last_ckpts: 3\n",
        "\n",
        "# model:\n",
        "#     initializer: \"xavier\"\n",
        "#     bias_initializer: \"zeros\"\n",
        "#     init_gain: 1.0\n",
        "#     embed_initializer: \"xavier\"\n",
        "#     embed_init_gain: 1.0\n",
        "#     tied_embeddings: True\n",
        "#     tied_softmax: True\n",
        "#     encoder:\n",
        "#         type: \"transformer\"\n",
        "#         num_layers: 6\n",
        "#         num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "#         embeddings:\n",
        "#             embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "#             scale: True\n",
        "#             dropout: 0.2\n",
        "#         # typically ff_size = 4 x hidden_size\n",
        "#         hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "#         ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "#         dropout: 0.3\n",
        "#     decoder:\n",
        "#         type: \"transformer\"\n",
        "#         num_layers: 6\n",
        "#         num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "#         embeddings:\n",
        "#             embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "#             scale: True\n",
        "#             dropout: 0.2\n",
        "#         # typically ff_size = 4 x hidden_size\n",
        "#         hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "#         ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "#         dropout: 0.3\n",
        "# \"\"\".format(name=lang_pair, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, \n",
        "#            target_language=target_language, model_type=model_type, use_cuda=use_cuda)\n",
        "\n",
        "# with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=lang_pair),'w') as f:\n",
        "#     f.write(config)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRYjx1eTZoA2"
      },
      "source": [
        "# # Train the model\n",
        "# # You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "# !cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiKWT9xFbyGq"
      },
      "source": [
        "# # Copy the created models from the notebook storage to google drive for persistant storage \n",
        "# !cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer_new/\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWwO3L2aeaT"
      },
      "source": [
        "# # Output our validation accuracy\n",
        "# ! cat \"$gdrive_path/models/${src}${tgt}_transformer_new/validations.txt\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzx_OGVZIt4G"
      },
      "source": [
        "!mkdir -p joeynmt/models/${src}${tgt}_transformer_new/ && cp -r \"$gdrive_path/models/\"* joeynmt/models/${src}${tgt}_transformer_new/\n",
        "!mkdir -p joeynmt/data/${src}${tgt}/ && cp -r \"$gdrive_path/data/\"* joeynmt/data/${src}${tgt}/\n",
        "!cp -r joeynmt/data/${src}${tgt}/{bpe.codes.5000,vocab.ru} ./"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbu7qqNb4ZF"
      },
      "source": [
        "# # Test our model\n",
        "# ! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/config.yaml\" --output_path \"$gdrive_path/models/predictions\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwk1hogjNFmF"
      },
      "source": [
        "def interactive_translate(text):\n",
        "  os.environ[\"config\"] = \"config.yaml\" if use_cuda else \"config_non_cuda.yaml\"\n",
        "  ! echo \"$text\" | sacremoses tokenize | sacremoses truecase -m \"$source_tc_model\" | subword-nmt apply-bpe -c \"$bpe_codes_file\" --vocabulary vocab.$source_language > \"joeynmt/in.txt\"\n",
        "  ! cd joeynmt; python3 -m joeynmt translate \"$gdrive_path/models/$config\" < in.txt 2> /dev/null | sacremoses detruecase 2> /dev/null | sacremoses detokenize 2> /dev/null | sed \"s/ '/'/\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-nZBP5gCoR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd0af993-8cc1-49d4-d478-e9347f1589e8"
      },
      "source": [
        "interactive_translate(\"Пусть мой комментарий затеряется, пусть утонет среди других.\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Няхай мой каментарый загубляецца, няхай патанне сярод іншых.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q46Kd6enki3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23749cd-cc58-44ae-b377-80b4a7f019ba"
      },
      "source": [
        "#interactive_translate(\"В Витебске кратковременный снег, мокрый снег.\")\n",
        "interactive_translate(\"В дневные часы 15 марта кое-где отмечались дожди.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "У дзённыя гадзіны 15 сакавіка я-дзе адзначаліся дажджы.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdm6L7sndJk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bc2a970-dea0-4a58-e924-a234c7c512f0"
      },
      "source": [
        "interactive_translate(\"Как объяснили медработники, сделано для того, чтобы защитить стены от возможных ударов каталок.\")\n",
        "interactive_translate(\"Также на первом этаже расположены различные диагностические службы.\")\n",
        "interactive_translate(\"Например кабинет компьютерной томографии.\")\n",
        "interactive_translate(\"Аппарат, с помощью которого проводят это информативное высокотехнологичное исследование, самый современный.\")\n",
        "interactive_translate(\"Рядом - изотопная лаборатория, где выполняют радионуклидную диагностику.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Як растлумачылі медработнікі, зроблена для таго, каб абараніць сцены ад магчымых удараў каталок.\n",
            "Таксама на першым паверсе размешчаны розныя дыягнастычныя службы.\n",
            "Напрыклад кабінет камп'ютарнай тамаграфіі.\n",
            "Апарат, з дапамогай якога праводзяць гэта інфарматыўнае высокатэхналагічнае даследаванне, самы сучасны.\n",
            "Побач — ізатопная лабараторыя, дзе выконваюць радыёнуклідную дыягнастыку.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZZTND06D4mh"
      },
      "source": [
        "# def file_translate(file):\n",
        "#   os.environ[\"config\"] = \"config.yaml\" if use_cuda else \"config_non_cuda.yaml\"\n",
        "#   ! sacremoses tokenize < \"$file\" | sacremoses truecase -m \"$source_tc_model\" | subword-nmt apply-bpe -c \"$bpe_codes_file\" --vocabulary vocab.$source_language > \"joeynmt/in.txt\"\n",
        "#   ! cd joeynmt; python3 -m joeynmt translate \"$gdrive_path/models/$config\" < in.txt 2> /dev/null | sacremoses detruecase 2> /dev/null | sacremoses detokenize 2> /dev/null | sed \"s/ '/'/\" > joey_pred.txt"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bykptFiU_rSw"
      },
      "source": [
        "# folder = f'{gdrive_path}/data/data_test/TestDatasets/AntonTestData'\n",
        "# in_file_name = \"2016-03_ru.txt\"\n",
        "# in_filepath = f'{folder}/{in_file_name}'\n",
        "# file_translate(in_filepath)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EnXirDVQZVD"
      },
      "source": [
        "# !cp -r joeynmt/joey_pred.txt \"$folder\"\n",
        "# !cp -r joeynmt/in.txt \"$folder\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4rS6fiXJQJ0"
      },
      "source": [
        "# folder = f'{gdrive_path}/data/data_test/TestDatasets/TsimafeiMiddleCorpusTest'\n",
        "# in_file_name = \"MiddleCorpusTestPart_Rus.txt\"\n",
        "# in_filepath = f'{folder}/{in_file_name}'\n",
        "# file_translate(in_filepath)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXADEkXkJdxn"
      },
      "source": [
        "# !cp -r joeynmt/joey_pred.txt \"$folder\"\n",
        "# !cp -r joeynmt/in.txt \"$folder\""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}